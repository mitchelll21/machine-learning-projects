---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

The goal of this project was to compare an MLP, XGBoost, and Random Forest model performance for identifying handwritten digits.


## Digit Identification

```{r}
library(tidyverse)
library(tidymodels)
library(janitor)
library(skimr)
library(vip)
```

## Import The Data

```{r}
mnist <- read_csv("digit_train.csv") %>%
   clean_names() %>%
  mutate(label = factor(label)) 

head(mnist)

```
## Train Test Split 
```{r}
train_test_spit<- initial_split(mnist, prop = 0.7)

train <- training(train_test_spit)
test  <- testing(train_test_spit)


# -- grey scale pixels are 0 - 255 
train_scaled <- train %>%
  mutate_if(is.numeric, funs(./255)) 

test_scaled <- test %>%
  mutate_if(is.numeric, funs(./255)) 

sprintf("Train PCT : %d", nrow(train))
sprintf("Test  PCT : %d", nrow(test))


split_data <- function(df, prop) {
  split <- initial_split(df, prop)
  
  train <- training(split)
  test <- testing(split)
  
  train_scale <- train %>%
    mutate_if(is.numeric, funs(./255)) 

  test_scale <- test %>%
    mutate_if(is.numeric, funs(./255)) 

  train_print <- sprintf("Train PCT : %d", nrow(train))
  test_print <- sprintf("Test  PCT : %d", nrow(test))

  print(train_print)
  print(test_print)
  
  train_scale
  test_scale
}

split_data(mnist, 0.07)

```



```{r}

# -- nothing to do except specify a formula 
mnist_recipe <- recipe(label ~ ., train_scaled) %>%
  step_rm(id)

mnist_mlp <- mlp(epochs = 10, hidden_units = 15) %>%
  set_engine("nnet", MaxNWts = 12255) %>%
  set_mode("classification") 

mnist_wf <- workflow() %>%
  add_recipe(mnist_recipe) %>%
  add_model(mnist_mlp) %>%
  fit(train_scaled)

```

```{r}
predict(mnist_wf, train_scaled,type="class") %>%
    bind_cols(., train )-> scored_train
    
predict(mnist_wf, test_scaled,type="class") %>%
    bind_cols(., test )-> scored_test

scored_test$label <- factor(scored_test$label,
levels = c(0,1,2,3,4,5,6,7,8,9),
labels = c("0", "1", "2", "3", "4", "5", "6", "7", "8", "9"))

scored_test$.pred_class <- factor(scored_test$.pred_class,
levels = c(0,1,2,3,4,5,6,7,8,9),
labels = c("0", "1", "2", "3", "4", "5", "6", "7", "8", "9"))

scored_train %>%
  mutate(.part = "train") %>%
  bind_rows(scored_test %>%
              mutate(.part = "test")
  ) %>%
  group_by(.part) %>%
    metrics(label, estimate = .pred_class) %>%
  filter(.metric == "accuracy") %>%
  spread(.metric, .estimate) %>%
  select(-.estimator)

 scored_test %>%
    conf_mat(label, .pred_class) %>%
    autoplot(type = "heatmap")
```
 

# visualize differences 

```{r}
digit_check <- function(id, scored) {
pixels_gathered <- scored %>%
  filter(label != .pred_class) %>%
  select(starts_with("x"), label,.pred_class ) %>%
  head(24) %>%
  mutate(instance = row_number()) %>%
  gather(pixel, value, -label, -instance, -.pred_class) %>%
  tidyr::extract(pixel, "pixel", "(\\d+)", convert = TRUE) %>%
  mutate(pixel = pixel - 2,
         x = pixel %% 28,
         y = 28 - pixel %/% 28)


theme_set(theme_light())

pixels_gathered %>%
  ggplot(aes(x, y, fill = value)) +
  geom_tile() +
  facet_wrap(~ label + .pred_class)
}

for(i in range(0:12)){
  p <- digit_check(i, scored_test)
  print(p)
}

```


##XGB Boost Model

```{r}

train_scaled <- train_scaled %>%
  sample_n(3500)

test_scaled <- test_scaled %>%
  sample_n(1500)

mnist_recipe <- recipe(label ~ ., train_scaled) %>%
  step_rm(id)

xgb_model <- boost_tree(trees = tune(), min_n = tune()) %>%
  set_mode("classification") %>%
  set_engine("xgboost")

xgb_model

xgb_workflow <- workflow() %>% 
  add_recipe(mnist_recipe) %>%
  add_model(xgb_model)

param_grid <- grid_regular(
  trees(),
  min_n(),
  levels = 2
)

param_grid

set.seed(96)
mnist_folds <- vfold_cv(train, v=3)
mnist_folds


set.seed(420)

doParallel::registerDoParallel() ## comment to turn parallel off 

xgb_rs <- tune_grid(
  xgb_workflow,
  resamples = mnist_folds,
  grid = param_grid, 
  control = control_grid(save_pred = TRUE, verbose = TRUE)
) 

xgb_rs
```

##Reviewed Tuning Results

```{r}
xgb_rs %>%
  collect_metrics()%>%
  mutate_if(is.numeric, round,3) %>% 
  pivot_wider(names_from = .metric, values_from=c(mean, std_err))
```

##Visualize Each Parameter

```{r}
xgb_rs %>%
  collect_metrics() %>%
  mutate_if(is.numeric, round,3) %>%
  ggplot(aes(tree_depth, mean, )) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2)

xgb_rs %>%
  collect_metrics()  %>%
  mutate_if(is.numeric, round,3) %>%
  ggplot(aes(min_n, mean, )) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2)

xgb_rs %>%
  collect_metrics()  %>%
  mutate_if(is.numeric, round,3) %>%
  ggplot(aes(learn_rate, mean, )) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2)
```


##Results and Select Best Parameters

```{r}
xgb_rs %>%
  show_best("roc_auc") %>%
  print()

xgb_best <- xgb_rs %>%
  select_best("roc_auc") 

print(xgb_best)
```
## refitting workflow with "best" parameters

```{r}
xgb_best_wf <- xgb_workflow %>%
  finalize_workflow(xgb_best)

print(xgb_best_wf)

xgb_best_fit  <- xgb_best_wf %>%
  fit(data = train)
```


```{r}

# -- score training  
options(yardstick.event_first = FALSE)


predict(xgb_best_fit, train, type="prob") %>%
bind_cols(
  predict(xgb_best_fit, train) %>%
    bind_cols(.,train)) -> scored_train_x

# -- score testing 
predict(xgb_best_fit, test, type="prob") %>%
  bind_cols(
      predict(xgb_best_fit, test) %>%
      bind_cols(., test)) -> scored_test_x   

# -- Metrics: Train and Test 
scored_train_x %>%
  mutate(.part = "train") %>%
  bind_rows(scored_test_x %>%
              mutate(.part = "test")
  ) %>%
  group_by(.part) %>%
    metrics(label, estimate = .pred_class) %>%
  filter(.metric == "accuracy") %>%
  spread(.metric, .estimate) %>%
  select(-.estimator)
  
```

```{r}
options(yardstick.event_first = FALSE)

  

 scored_test_x %>%
    conf_mat(label, .pred_class) %>%
    autoplot(type = "heatmap")



```
# visualize differences 

```{r}

for(i in range(0:12)){
  p <- digit_check(i, scored_test_x)
  print(p)
}
```

## Random Forest

```{r}

tune_grid <- grid_regular(trees(c(200, 400)),
                          min_n(),
                          levels = 3)



rf_model <- rand_forest(trees = tune(),
                            min_n = tune()) %>%  
  set_engine('ranger', importance = 'permutation') %>%
  set_mode('classification')

rf_wf <- workflow() %>%
  add_recipe(mnist_recipe) %>%
  add_model(rf_model)

rf_tune_results <- rf_wf %>%
  tune_grid(
    resamples = mnist_folds,
    grid = tune_grid
  )


```

## Review Tuning Results 
```{r}
## -- results of tuning -- 
rf_tune_results %>% 
  collect_metrics() %>%
  mutate_if(is.numeric, round,3) %>% 
  pivot_wider(names_from = .metric, values_from=c(mean, std_err))
```
## Visualize impact 


```{r}
## - visualize 
rf_tune_results %>%
  collect_metrics() %>%
  mutate_if(is.numeric, round,3) %>%
  ggplot(aes(trees, mean, )) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) 

rf_tune_results %>%
  collect_metrics()  %>%
  mutate_if(is.numeric, round,3) %>%
  ggplot(aes(min_n, mean, )) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) 

```

## results 
selecting "best" parameters
```{r}
rf_tune_results %>%
  show_best("roc_auc") %>%
  print()

rf_best <- rf_tune_results %>%
  select_best("roc_auc") 

print(rf_best)
```

## refitting workflow with "best" parameters

```{r}
rf_final_wf <- rf_wf %>% 
  finalize_workflow(rf_best)

print(rf_final_wf)

rf_final_fit  <- rf_final_wf %>%
  fit(data = train) 
```

```{r}
# -- score training  
predict(rf_final_fit, train, type = 'prob') %>%
  bind_cols(predict(rf_final_fit, train, type = 'class')) %>%
  bind_cols(.,train)-> scored_train 

# -- score testing 
predict(rf_final_fit, test, type = 'prob') %>%
  bind_cols(predict(rf_final_fit, test, type = 'class')) %>%
     bind_cols(., test) -> scored_test2   

# -- Metrics: Train and Test 
scored_train %>%
  mutate(.part = "train") %>%
  bind_rows(scored_test2 %>%
              mutate(.part = "test")
  ) %>%
  group_by(.part) %>%
    metrics(label, estimate = .pred_class) %>%
  filter(.metric == "accuracy") %>%
  spread(.metric, .estimate) %>%
  select(-.estimator)

 scored_test2 %>%
    conf_mat(label, .pred_class) %>%
    autoplot(type = "heatmap")
  
```

# visualize differences 

```{r}

for(i in range(0:12)){
  p <- digit_check(i, scored_test2)
  print(p)
}
```



```{r}

bind_rows(scored_test2 %>%
            mutate(model = 'random forest'),
          scored_test %>%
            mutate(model = 'MLP'),
          scored_test_x %>%
            mutate(model = 'XGBoost')) %>%
  group_by(model) %>%
  metrics(label, estimate = .pred_class) %>%
  pivot_wider(id_cols = model, values_from = .estimate, names_from = .metric)



```



##Kaggle
```{r}
digit_kaggle <- read_csv('digit_holdout.csv')

digit_kaggle <- digit_kaggle %>%
  clean_names()

kaggle_p5 <- predict(mnist_wf, digit_kaggle, type = 'class') %>%
  bind_cols(digit_kaggle) %>%
  dplyr::select(id, label = .pred_class)

kaggle_p52 <- predict(rf_final_fit, digit_kaggle, type = 'class') %>%
  bind_cols(digit_kaggle) %>%
  dplyr::select(id, label = .pred_class)

kaggle_p53 <- predict(xgb_best_fit, digit_kaggle, type = 'class') %>%
  bind_cols(digit_kaggle) %>%
  dplyr::select(id, label = .pred_class)

write_csv(kaggle_p5, 'kaggle_p5.csv')

write_csv(kaggle_p52, 'kaggle_p52.csv')

write_csv(kaggle_p53, 'kaggle_p53.csv')

```




