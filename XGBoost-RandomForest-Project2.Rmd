---
title: "Project4"
output: html_document
---

Our goal was to build a predictive model to predict 'bad buys' from used car sale data. Gradient Boosting and Random Forest models were tuned to find the best parameters. 

```{r}
options (scipen = 999)

install.packages('tidyverse')
install.packages('tidymodels')
install.packages('janitor')
install.packages('skimr')
install.packages('vip')
install.packages('doParallel')
install.packages('textrecipes')
install.packages('xgboost')
install.packages('stopwords')
install.packages('ranger')


library(tidyverse)
library(tidymodels)
library(janitor)
library(skimr)
library(vip)
library(doParallel)
library(textrecipes)
library(xgboost)
library(stopwords)
library(ranger)
```

```{r}
car <- read_csv("project_4_training-2.csv") %>% clean_names() %>% mutate(is_bad_buy = factor(is_bad_buy))
car %>% head()

car_kaggle <- read_csv('project_4_kaggle-2.csv')
```

```{r}

skim(car)

```

```{r}
car %>%
  group_by(is_bad_buy) %>%
  summarize(n=n()) %>%
  ungroup() %>%
  mutate(pct = n/sum(n))

car %>%
  group_by(has_company_logo) %>%
  filter(is_bad_buy == '1') %>%
  summarize(n=n()) %>%
  ungroup() %>%
  mutate(pct = n/sum(n))

car %>%
  group_by(has_questions) %>%
  filter(is_bad_buy == '1') %>%
  summarize(n=n()) %>%
  ungroup() %>%
  mutate(pct = n/sum(n))
```



```{r}
set.seed(6969)


car_split <- initial_split(car, prop = 0.7, strata = is_bad_buy)

train <- training(car_split)
test <- testing(car_split)

sprintf("Train PCT : %1.2f%%", nrow(train)/ nrow(car) * 100)
sprintf("Test  PCT : %1.2f%%", nrow(test)/ nrow(car) * 100)

```
##XGB Boost Model

```{r}


car_recipe <- recipe(is_bad_buy ~ ., 
                     train) %>%
  step_rm(id, purch_date, model, vnst, byrno, aucguart) %>%
  step_impute_mean(all_numeric_predictors())%>%
  step_impute_mode(all_nominal_predictors())%>%
  step_dummy(all_nominal_predictors()) %>%
  prep()


xgb_model <- boost_tree(trees = tune(), min_n = tune()) %>%
  set_mode("classification") %>%
  set_engine("xgboost")

xgb_model

xgb_workflow <- workflow() %>% 
  add_recipe(car_recipe) %>%
  add_model(xgb_model)

param_grid <- grid_regular(
  trees(),
  min_n(),
  levels = 2
)

param_grid

set.seed(96)
car_folds <- vfold_cv(train, v=5)
car_folds


set.seed(420)

doParallel::registerDoParallel() ## comment to turn parallel off 

xgb_rs <- tune_grid(
  xgb_workflow,
  resamples = car_folds,
  grid = param_grid, 
  control = control_grid(save_pred = TRUE, verbose = TRUE)
) 

xgb_rs
```

##Reviewed Tuning Results

```{r}
xgb_rs %>%
  collect_metrics()%>%
  mutate_if(is.numeric, round,3) %>% 
  pivot_wider(names_from = .metric, values_from=c(mean, std_err))
```

##Visualize Each Parameter

```{r}
xgb_rs %>%
  collect_metrics() %>%
  mutate_if(is.numeric, round,3) %>%
  ggplot(aes(tree_depth, mean, )) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2)

xgb_rs %>%
  collect_metrics()  %>%
  mutate_if(is.numeric, round,3) %>%
  ggplot(aes(min_n, mean, )) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2)

xgb_rs %>%
  collect_metrics()  %>%
  mutate_if(is.numeric, round,3) %>%
  ggplot(aes(learn_rate, mean, )) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2)
```


##Results and Select Best Parameters

```{r}
xgb_rs %>%
  show_best("roc_auc") %>%
  print()

xgb_best <- xgb_rs %>%
  select_best("roc_auc") 

print(xgb_best)
```
## refitting workflow with "best" parameters

```{r}
xgb_best_wf <- xgb_workflow %>%
  finalize_workflow(xgb_best)

print(xgb_best_wf)

xgb_best_fit  <- xgb_best_wf %>%
  fit(data = train)
```


```{r}
xgb_best_fit %>% 
  pull_workflow_fit() %>% 
  vip(20)
```


```{r}

# -- score training  
options(yardstick.event_first = FALSE)


predict(xgb_best_fit, train, type="prob") %>%
bind_cols(
  predict(xgb_best_fit, train) %>%
    bind_cols(.,train)) -> scored_train_x

# -- score testing 
predict(xgb_best_fit, test, type="prob") %>%
  bind_cols(
      predict(xgb_best_fit, test) %>%
      bind_cols(., test)) -> scored_test_x   

# -- Metrics: Train and Test 
scored_train_x %>% 
  metrics(is_bad_buy, estimate = .pred_class, .pred_1) %>%
  mutate(part="training") %>%
  bind_rows( scored_test_x %>% 
                 metrics(is_bad_buy, estimate = .pred_class, .pred_1) %>%
               mutate(part="testing") ) %>%
  pivot_wider(names_from = .metric, values_from=.estimate)
  
# -- variable importance: top 10
xgb_best_fit %>%
  pull_workflow_fit() %>%
  vip(num_features = 10)

  
```

```{r}
options(yardstick.event_first = FALSE)

  

scored_train_x %>% 
  mutate(part="training") %>%
  bind_rows( scored_test_x %>% 
               mutate(part="testing") ) %>%
 group_by(part) %>%
 roc_curve(is_bad_buy, .pred_1) %>%
  autoplot()



```


## Random Forest

```{r}

tune_grid <- grid_regular(trees(c(100, 300)),
                          min_n(),
                          levels = 2)

rf_model <- rand_forest(trees = tune(),
                            min_n = tune()) %>%  
  set_engine('ranger', importance = 'permutation') %>%
  set_mode('classification')

rf_wf <- workflow() %>%
  add_recipe(car_recipe) %>%
  add_model(rf_model)

rf_tune_results <- rf_wf %>%
  tune_grid(
    resamples = car_folds,
    grid = tune_grid
  )


```

## Review Tuning Results 
```{r}
## -- results of tuning -- 
rf_tune_results %>% 
  collect_metrics() %>%
  mutate_if(is.numeric, round,3) %>% 
  pivot_wider(names_from = .metric, values_from=c(mean, std_err))
```
## Visualize impact 


```{r}
## - visualize 
rf_tune_results %>%
  collect_metrics() %>%
  mutate_if(is.numeric, round,3) %>%
  ggplot(aes(trees, mean, )) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) 

rf_tune_results %>%
  collect_metrics()  %>%
  mutate_if(is.numeric, round,3) %>%
  ggplot(aes(min_n, mean, )) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) 

```

## results 
selecting "best" parameters
```{r}
rf_tune_results %>%
  show_best("roc_auc") %>%
  print()

rf_best <- rf_tune_results %>%
  select_best("roc_auc") 

print(rf_best)
```

## refitting workflow with "best" parameters

```{r}
rf_final_wf <- rf_wf %>% 
  finalize_workflow(rf_best)

print(rf_final_wf)

rf_final_fit  <- rf_final_wf %>%
  fit(data = train) 
```

```{r}
rf_final_fit %>% 
  pull_workflow_fit() %>% 
  vip(20)
```

```{r}
# -- score training  
predict(rf_final_fit, train, type = 'prob') %>%
  bind_cols(predict(rf_final_fit, train, type = 'class')) %>%
  bind_cols(.,train)-> scored_train 

# -- score testing 
predict(rf_final_fit, test, type = 'prob') %>%
  bind_cols(predict(rf_final_fit, test, type = 'class')) %>%
     bind_cols(., test) -> scored_test2   

# -- Metrics: Train and Test 
scored_train %>% 
  metrics(is_bad_buy, .pred_1, estimate = .pred_class) %>%
  mutate(part = "training") %>%
  bind_rows(scored_test2 %>% 
               metrics(is_bad_buy, .pred_1, estimate = .pred_class) %>%
               mutate(part="testing") ) %>%
  filter(.metric %in% c('accuracy', 'roc_auc')) %>%
  pivot_wider(names_from = .metric, values_from=.estimate)


# -- variable importance: top 10
rf_final_fit %>%
  pull_workflow_fit() %>%
  vip(num_features = 10)



  
```

```{r}

bind_rows(scored_test2 %>%
            mutate(model = 'random forest'),
          scored_test_x %>%
            mutate(model = 'XGBoost')) %>%
  group_by(model) %>%
  metrics(is_bad_buy, estimate = .pred_class, .pred_1) %>%
  pivot_wider(id_cols = model, values_from = .estimate, names_from = .metric)


bind_rows(scored_test2 %>%
            mutate(model = 'random forest'),
          scored_test_x %>%
            mutate(model = 'XGBoost')) %>%
  group_by(model) %>%
  roc_curve(is_bad_buy, .pred_1) %>%
  autoplot() +
  labs(title = 'ROC chart')

```



##Kaggle
```{r}

car_kaggle <- car_kaggle %>%
  clean_names()

kaggle_p4 <- predict(xgb_best_fit, car_kaggle, type = 'prob') %>%
  bind_cols(car_kaggle) %>%
  mutate(IsBadBuy = .pred_1) %>%
  dplyr::select(id,IsBadBuy)

kaggle_p42 <- predict(rf_final_fit, car_kaggle, type = 'prob') %>%
  bind_cols(car_kaggle) %>%
  mutate(IsBadBuy = .pred_1) %>%
  dplyr::select(id,IsBadBuy)

write_csv(kagglep4, 'kaggle_p4.csv')

write_csv(kagglep4, 'kaggle_p42.csv')


```
