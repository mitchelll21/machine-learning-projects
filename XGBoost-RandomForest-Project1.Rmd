---
title: "R Notebook"
output: html_notebook
---


Our goal is to identify fraudulent job postings from text analysis and predicitve modeling. Gradient Boosting and Random Forest Models were tuned to find the best parameters.


```{r}
options (scipen = 999)

install.packages('tidyverse')
install.packages('tidymodels')
install.packages('janitor')
install.packages('skimr')
install.packages('vip')
install.packages('doParallel')
install.packages('textrecipes')
install.packages('xgboost')
install.packages('stopwords')
install.packages('ranger')


library(tidyverse)
library(tidymodels)
library(janitor)
library(skimr)
library(vip)
library(doParallel)
library(textrecipes)
library(xgboost)
library(stopwords)
library(ranger)
```

```{r}
jobs <- read_csv("job_training.csv") %>% clean_names() %>% mutate(fraudulent = factor(fraudulent))
jobs %>% head()

job_holdout <- read_csv('job_holdout.csv')
```


```{r}
jobs %>%
  group_by(fraudulent) %>%
  summarize(n=n()) %>%
  ungroup() %>%
  mutate(pct = n/sum(n))

jobs %>%
  group_by(has_company_logo) %>%
  filter(fraudulent == '1') %>%
  summarize(n=n()) %>%
  ungroup() %>%
  mutate(pct = n/sum(n))

jobs %>%
  group_by(has_questions) %>%
  filter(fraudulent == '1') %>%
  summarize(n=n()) %>%
  ungroup() %>%
  mutate(pct = n/sum(n))
```



```{r}
set.seed(6969)


jobs_split <- initial_split(jobs,prop = 0.7, strata = fraudulent)

train <- training(jobs_split)
test <- testing(jobs_split)

sprintf("Train PCT : %1.2f%%", nrow(train)/ nrow(jobs) * 100)
sprintf("Test  PCT : %1.2f%%", nrow(test)/ nrow(jobs) * 100)

```
##XGB Boost Model

```{r}
jobs <- jobs %>%
  mutate(has_company_logo = as.character(has_company_logo),
         has_questions = as.character(has_questions))

job_recipe <- recipe(fraudulent ~ description + company_profile + 
                       has_company_logo + has_questions, 
                     train) %>%
   step_indicate_na(has_company_logo) %>%
  step_tokenize(description , company_profile) %>%
  step_stopwords(description, company_profile) %>%
  step_ngram(description,company_profile, num_tokens = 2, min_num_tokens = 1) %>%
  step_tokenfilter(description,company_profile, max_tokens = 100, min_times = 5) %>%
  step_tfidf(description, company_profile)


xgb_model <- boost_tree(trees = tune(),tree_depth = tune(), min_n = tune(), learn_rate = tune() ) %>%
  set_mode("classification") %>%
  set_engine("xgboost")

xgb_model

xgb_workflow <- workflow() %>% 
  add_recipe(job_recipe) %>%
  add_model(xgb_model)

param_grid <- grid_regular(
  trees(),
  tree_depth(),
  min_n(),
  learn_rate(),
  levels = 2
)

param_grid

set.seed(96)
job_folds <- vfold_cv(train, v=3)
job_folds


set.seed(420)

doParallel::registerDoParallel() ## comment to turn parallel off 

xgb_rs <- tune_grid(
  xgb_workflow,
  resamples = job_folds,
  grid = param_grid, 
  control = control_grid(save_pred = TRUE, verbose = TRUE)
) 

xgb_rs
```

##Reviewed Tuning Results

```{r}
xgb_rs %>%
  collect_metrics()%>%
  mutate_if(is.numeric, round,3) %>% 
  pivot_wider(names_from = .metric, values_from=c(mean, std_err))
```

##Visualize Each Parameter

```{r}
xgb_rs %>%
  collect_metrics() %>%
  mutate_if(is.numeric, round,3) %>%
  ggplot(aes(tree_depth, mean, )) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2)

xgb_rs %>%
  collect_metrics()  %>%
  mutate_if(is.numeric, round,3) %>%
  ggplot(aes(min_n, mean, )) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2)

xgb_rs %>%
  collect_metrics()  %>%
  mutate_if(is.numeric, round,3) %>%
  ggplot(aes(learn_rate, mean, )) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2)
```


##Results and Select Best Parameters

```{r}
xgb_rs %>%
  show_best("roc_auc") %>%
  print()

xgb_best <- xgb_rs %>%
  select_best("roc_auc") 

print(xgb_best)
```
## refitting workflow with "best" parameters

```{r}
xgb_best_wf <- xgb_workflow %>%
  finalize_workflow(xgb_best)

print(xgb_best_wf)

xgb_best_fit  <- xgb_best_wf %>%
  fit(data = train)
```


```{r}
xgb_best_fit %>% 
  pull_workflow_fit() %>% 
  vip(20)
```


```{r}

# -- score training  
options(yardstick.event_first = FALSE)


predict(xgb_best_fit, train, type="prob") %>%
bind_cols(
  predict(xgb_best_fit, train) %>%
    bind_cols(.,train)) -> scored_train_x

# -- score testing 
predict(xgb_best_fit, test, type="prob") %>%
  bind_cols(
      predict(xgb_best_fit, test) %>%
      bind_cols(., test)) -> scored_test_x   

# -- Metrics: Train and Test 
scored_train %>% 
  metrics(fraudulent, estimate = .pred_class, .pred_1) %>%
  mutate(part="training") %>%
  bind_rows( scored_test %>% 
                 metrics(fraudulent, estimate = .pred_class, .pred_1) %>%
               mutate(part="testing") ) %>%
  pivot_wider(names_from = .metric, values_from=.estimate)
  
# -- variable importance: top 10
xgb_best_fit %>%
  pull_workflow_fit() %>%
  vip(num_features = 10)

  
```

```{r}
options(yardstick.event_first = FALSE)

  

scored_train %>% 
  mutate(part="training") %>%
  bind_rows( scored_test %>% 
               mutate(part="testing") ) %>%
 group_by(part) %>%
 roc_curve(fraudulent, .pred_1) %>%
  autoplot()



```


## Random Forest

```{r}

tune_grid <- grid_regular(trees(c(10, 150)),
                          min_n(),
                          levels = 2)

rf_model <- rand_forest(trees = tune(),
                            min_n = tune()) %>%  
  set_engine('ranger', importance = 'permutation') %>%
  set_mode('classification')

rf_wf <- workflow() %>%
  add_recipe(job_recipe) %>%
  add_model(rf_model)

rf_tune_results <- rf_wf %>%
  tune_grid(
    resamples = job_folds,
    grid = tune_grid
  )


```

## Review Tuning Results 
```{r}
## -- results of tuning -- 
rf_tune_results %>% 
  collect_metrics() %>%
  mutate_if(is.numeric, round,3) %>% 
  pivot_wider(names_from = .metric, values_from=c(mean, std_err))
```
## Visualize impact 


```{r}
## - visualize 
rf_tune_results %>%
  collect_metrics() %>%
  mutate_if(is.numeric, round,3) %>%
  ggplot(aes(trees, mean, )) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) 

rf_tune_results %>%
  collect_metrics()  %>%
  mutate_if(is.numeric, round,3) %>%
  ggplot(aes(min_n, mean, )) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) 

```

## results 
selecting "best" parameters
```{r}
rf_tune_results %>%
  show_best("roc_auc") %>%
  print()

rf_best <- rf_tune_results %>%
  select_best("roc_auc") 

print(rf_best)
```

## refitting workflow with "best" parameters

```{r}
rf_final_wf <- rf_wf %>% 
  finalize_workflow(rf_best)

print(rf_final_wf)

rf_final_fit  <- rf_final_wf %>%
  fit(data = train) 
```

```{r}
rf_final_fit %>% 
  pull_workflow_fit() %>% 
  vip(20)
```

```{r}
# -- score training  
predict(rf_final_fit, train, type = 'prob') %>%
  bind_cols(predict(rf_final_fit, train, type = 'class')) %>%
  bind_cols(.,train)-> scored_train 

# -- score testing 
predict(rf_final_fit, test, type = 'prob') %>%
  bind_cols(predict(rf_final_fit, test, type = 'class')) %>%
     bind_cols(., test) -> scored_test2   

# -- Metrics: Train and Test 
scored_train %>% 
  metrics(fraudulent, .pred_1, estimate = .pred_class) %>%
  mutate(part = "training") %>%
  bind_rows(scored_test2 %>% 
               metrics(fraudulent, .pred_1, estimate = .pred_class) %>%
               mutate(part="testing") ) %>%
  filter(.metric %in% c('accuracy', 'roc_auc')) %>%
  pivot_wider(names_from = .metric, values_from=.estimate)


# -- variable importance: top 10
rf_final_fit %>%
  pull_workflow_fit() %>%
  vip(num_features = 10)



  
```

```{r}

bind_rows(scored_test2 %>%
            mutate(model = 'random forest'),
          scored_test_x %>%
            mutate(model = 'XGBoost')) %>%
  group_by(model) %>%
  metrics(fraudulent, estimate = .pred_class, .pred_1) %>%
  pivot_wider(id_cols = model, values_from = .estimate, names_from = .metric)


bind_rows(scored_test2 %>%
            mutate(model = 'random forest'),
          scored_test_x %>%
            mutate(model = 'XGBoost')) %>%
  group_by(model) %>%
  roc_curve(fraudulent, .pred_1) %>%
  autoplot() +
  labs(title = 'ROC chart')

```



##Kaggle
```{r}
kaggle <- predict(xgb_best_fit, job_holdout, type = 'prob') %>%
  bind_cols(job_holdout) %>%
  mutate(fraudulent = .pred_1) %>%
  dplyr::select(job_id, fraudulent)

kaggle2 <- predict(rf_final_fit, job_holdout, type = 'prob') %>%
  bind_cols(job_holdout) %>%
  mutate(fraudulent = .pred_1) %>%
  dplyr::select(job_id, fraudulent)

write_csv(kaggle2, 'kaggle2.csv')




```
